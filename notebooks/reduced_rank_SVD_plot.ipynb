{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af08aec5-0555-426b-8d5a-4509e5a7adf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.linalg import svd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from side_info_decoding.utils import (\n",
    "    set_seed, \n",
    "    load_data_from_pids, \n",
    "    sliding_window_over_trials\n",
    ")\n",
    "from side_info_decoding.reduced_rank import (\n",
    "    Full_Rank_Model,\n",
    "    Multi_Task_Reduced_Rank_Model, \n",
    "    train_multi_task, \n",
    "    model_eval\n",
    ")\n",
    "\n",
    "from one.api import ONE\n",
    "\n",
    "seed = 666\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ed8100-17c8-4389-b515-b6e5c317e956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rc(\"figure\", dpi=100)\n",
    "SMALL_SIZE = 10\n",
    "BIGGER_SIZE = 15\n",
    "plt.rc('font', size=BIGGER_SIZE)\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)\n",
    "plt.rc('axes', linewidth=2)\n",
    "plt.rc('xtick', labelsize=BIGGER_SIZE)\n",
    "plt.rc('ytick', labelsize=BIGGER_SIZE)\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)\n",
    "plt.rc('figure', titlesize=2)\n",
    "plt.rcParams['xtick.major.size'] = 10\n",
    "plt.rcParams['xtick.minor.size'] = 10\n",
    "plt.rcParams['ytick.major.size'] = 10\n",
    "plt.rcParams['ytick.minor.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af89aad4-487d-4430-a5e4-a3b8ad9036ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# regions = [\n",
    "#     \"CP\", \"GPe\", \"LSr\",\n",
    "#     \"PO\", \"DG\", \"LP\",\n",
    "#     \"NI\", \"PB\", \"PAG\",\n",
    "#     \"SCm\", \"SNr\", \"IRN\",\n",
    "#     \"SPIV\", \"LGv\", \"LIN\", \"MDRN\",\n",
    "#     \"PYR\", \"COPY\", \"VAL\",\n",
    "#     \"ORBvl\", \"Alv\", \"FRP\",\n",
    "#     \"STN\", \"APr\"\n",
    "# ]\n",
    "\n",
    "regions = [\"LP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941ef45b-9433-43cd-8b6c-b83485005cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 regions remaining ...\n",
      "LP: 111 PIDs\n",
      "111 regions available ...\n",
      "pulling data from ibl database ..\n",
      "eid: ebce500b-c530-47de-8cb1-963c552703ea\n",
      "pid: 8c732bf2-639d-496c-bf82-464bc9c2d54b\n",
      "number of trials found: 470\n",
      "found 470 trials from 13.74 to 5761.52 sec.\n",
      "found 139 Kilosort units in region lp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute spike count: 100%|███████████████████| 470/470 [00:01<00:00, 353.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling data from ibl database ..\n",
      "eid: 15b69921-d471-4ded-8814-2adad954bcd8\n",
      "pid: 7a620688-66cb-44d3-b79b-ccac1c8ba23e\n",
      "number of trials found: 715\n",
      "found 715 trials from 28.03 to 3547.82 sec.\n",
      "found 47 Kilosort units in region lp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute spike count: 100%|██████████████████| 715/715 [00:00<00:00, 2825.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling data from ibl database ..\n",
      "eid: ebce500b-c530-47de-8cb1-963c552703ea\n",
      "pid: 8c732bf2-639d-496c-bf82-464bc9c2d54b\n",
      "number of trials found: 470\n",
      "found 470 trials from 13.74 to 5761.52 sec.\n",
      "found 34 good units in region lp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute spike count: 100%|██████████████████| 470/470 [00:00<00:00, 1674.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling data from ibl database ..\n",
      "eid: 15b69921-d471-4ded-8814-2adad954bcd8\n",
      "pid: 7a620688-66cb-44d3-b79b-ccac1c8ba23e\n",
      "number of trials found: 715\n",
      "found 715 trials from 28.03 to 3547.82 sec.\n",
      "found 0 good units in region lp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute spike count: 100%|█████████████████| 715/715 [00:00<00:00, 93939.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 folds remaining ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [700/7000], Loss: 9.140467170355361\n",
      "Epoch [1400/7000], Loss: 1.1869052034502268\n",
      "Epoch [2100/7000], Loss: 0.49463793638201414\n",
      "Epoch [2800/7000], Loss: 0.4287902326694561\n",
      "Epoch [3500/7000], Loss: 0.4120512491609508\n",
      "Epoch [4200/7000], Loss: 0.3971540389101153\n",
      "Epoch [4900/7000], Loss: 0.3870029015131694\n",
      "Epoch [5600/7000], Loss: 0.38272623933359967\n",
      "Epoch [6300/7000], Loss: 0.38121903186876477\n",
      "Epoch [7000/7000], Loss: 0.38008190209686565\n",
      "task 0 train accuracy: 0.947 auc: 0.992\n",
      "task 0 test accuracy: 0.670 auc: 0.706\n",
      "task 1 train accuracy: 0.752 auc: 0.855\n",
      "task 1 test accuracy: 0.706 auc: 0.718\n",
      "Epoch [700/7000], Loss: 9.86449275978176\n",
      "Epoch [1400/7000], Loss: 3.173992456499351\n",
      "Epoch [2100/7000], Loss: 0.6266558590645952\n",
      "Epoch [2800/7000], Loss: 0.2937889797970517\n",
      "Epoch [3500/7000], Loss: 0.19406069972534917\n",
      "Epoch [4200/7000], Loss: 0.15836737360349404\n",
      "Epoch [4900/7000], Loss: 0.1499203515810612\n",
      "Epoch [5600/7000], Loss: 0.14876895269221502\n",
      "Epoch [6300/7000], Loss: 0.1486678034547296\n",
      "Epoch [7000/7000], Loss: 0.1486671712982498\n",
      "task 0 train accuracy: 1.000 auc: 1.000\n",
      "task 0 test accuracy: 0.723 auc: 0.799\n",
      "task 1 train accuracy: 0.984 auc: 1.000\n",
      "task 1 test accuracy: 0.671 auc: 0.678\n",
      "not enough values to unpack (expected 4, got 3)\n"
     ]
    }
   ],
   "source": [
    "for re_idx, roi in enumerate(regions):\n",
    "    \n",
    "    print(f\"{re_idx+1}/{len(regions)} regions remaining ...\")\n",
    "    \n",
    "    try:\n",
    "        bwm_session_file = \"/mnt/3TB/yizi/decode-paper-brain-wide-map/decoding/bwm_cache_sessions.pqt\"\n",
    "\n",
    "        bwm_df = pd.read_parquet(bwm_session_file)\n",
    "\n",
    "        one = ONE(base_url=\"https://openalyx.internationalbrainlab.org\", mode='remote')\n",
    "        pids_per_region = one.search_insertions(atlas_acronym=[roi], query_type='remote')\n",
    "        print(f\"{roi}: {len(pids_per_region)} PIDs\")\n",
    "\n",
    "        print(f\"{len(pids_per_region)} regions available ...\")\n",
    "        pids = list(pids_per_region)[:2]\n",
    "\n",
    "        all_svd_V = {}\n",
    "\n",
    "        X_dict, Y_dict = load_data_from_pids(\n",
    "            pids,\n",
    "            brain_region=roi.lower(),\n",
    "            behavior=\"choice\",\n",
    "            data_type=\"all_ks\",\n",
    "            n_t_bins = 40,\n",
    "            t_before=0.5,\n",
    "            t_after=1.5,\n",
    "            align_time_type='stimOn_times',\n",
    "        )\n",
    "\n",
    "        # load contrast\n",
    "        _, contrast_dict = load_data_from_pids(\n",
    "            pids,\n",
    "            brain_region=roi.lower(),\n",
    "            behavior=\"contrast\",\n",
    "            data_type=\"good_ks\",\n",
    "            n_t_bins = 40,\n",
    "            t_before=0.5,\n",
    "            t_after=1.5,\n",
    "            align_time_type='stimOn_times',\n",
    "        )\n",
    "\n",
    "        loaded_pids = list(X_dict.keys())\n",
    "\n",
    "        contrast_level_dict = {}\n",
    "        filter_trials_dict = {}\n",
    "        for pid in loaded_pids:\n",
    "            contrast_dict[pid] = np.nan_to_num(contrast_dict[pid], 0)\n",
    "            contrast_dict[pid].T[0] *= -1\n",
    "            contrast_level_dict[pid] = contrast_dict[pid].sum(1)\n",
    "            filter_trials_dict[pid] = {}\n",
    "            for level in np.unique(contrast_level_dict[pid]):\n",
    "                filter_trials_dict[pid].update({level: np.argwhere(contrast_level_dict[pid] == level).flatten()})\n",
    "            for val in np.unique(Y_dict[pid]):\n",
    "                if val == 0:\n",
    "                    direc = \"L\"\n",
    "                else:\n",
    "                    direc = \"R\"\n",
    "                filter_trials_dict[pid].update({direc: np.argwhere(Y_dict[pid] == val).flatten()})\n",
    "\n",
    "        R = 2 # rank\n",
    "        d = 0 # half window size\n",
    "        n_epochs = 7000            \n",
    "        n_folds = 5\n",
    "\n",
    "        train_pids, n_units = [], []\n",
    "        train_X_dict, train_Y_dict = {}, {}\n",
    "        for pid in loaded_pids:\n",
    "            X, Y = X_dict[pid], Y_dict[pid]\n",
    "            K, C, T = X.shape\n",
    "            if C < 10:\n",
    "                continue\n",
    "            train_pids.append(pid)\n",
    "            n_units.append(C)\n",
    "            X = sliding_window_over_trials(X, half_window_size=d)\n",
    "            Y = sliding_window_over_trials(Y, half_window_size=d)\n",
    "            X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "            train_X_dict.update({pid: X})\n",
    "            train_Y_dict.update({pid: Y})\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "#         # extract V from trials with different contrasts\n",
    "#         print(\"Start extracting V from trials with different contrasts ...\")\n",
    "#         train_X_lst = [train_X_dict[pid] for pid in train_pids]\n",
    "#         train_Y_lst = [train_Y_dict[pid] for pid in train_pids]\n",
    "\n",
    "#         multi_task_rrm = Multi_Task_Reduced_Rank_Model(\n",
    "#             n_tasks=len(train_pids),\n",
    "#             n_units=n_units, \n",
    "#             n_t_bins=T, \n",
    "#             rank=R, \n",
    "#             half_window_size=d,\n",
    "#             init_Us = None,\n",
    "#             init_V = None,\n",
    "#         )\n",
    "\n",
    "#         # training on all data\n",
    "#         rrm, train_losses = train_multi_task(\n",
    "#             model=multi_task_rrm,\n",
    "#             train_dataset=(train_X_lst, train_Y_lst),\n",
    "#             test_dataset=(train_X_lst, train_Y_lst),\n",
    "#             loss_function=torch.nn.BCELoss(),\n",
    "#             learning_rate=1e-3,\n",
    "#             weight_decay=1e-1,\n",
    "#             n_epochs=n_epochs,\n",
    "#         )\n",
    "\n",
    "#         init_Us = np.array([multi_task_rrm.Us[pid_idx].detach().numpy() for pid_idx in range(len(train_pids))])\n",
    "#         init_V = multi_task_rrm.V.detach().numpy()\n",
    "#         Us, Vs = {}, {}\n",
    "#         for pid_idx, pid in enumerate(train_pids):\n",
    "#             Us.update({pid: init_Us[pid_idx]})\n",
    "#             Vs.update({pid: init_V})\n",
    "\n",
    "#         svd_W, svd_U, svd_S, svd_VT, S_mul_VT, W_reduced = [], [], [], [], [], []\n",
    "#         for pid in train_pids:\n",
    "#             W = np.array(Us[pid]) @ np.array(Vs[pid]).squeeze()\n",
    "#             U, S, VT = svd(W)\n",
    "#             svd_W.append(W)\n",
    "#             svd_U.append(U[:, :R])\n",
    "#             svd_S.append(S[:R])\n",
    "#             svd_VT.append(VT[:R, :])\n",
    "#             if len(S) == 1:\n",
    "#                 S_mul_VT.append(np.diag(S) @ VT[:1, :])\n",
    "#             else:\n",
    "#                 S_mul_VT.append(np.diag(S[:R]) @ VT[:R, :])\n",
    "\n",
    "#         all_svd_V.update({\"all\": S_mul_VT})\n",
    "\n",
    "#         plt.figure()\n",
    "#         plt.plot(np.abs(np.array(all_svd_V[\"all\"])).mean(0).T[:,0])\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "#         # trials with diff choices\n",
    "#         for direc in [\"L\", \"R\"]:\n",
    "#             test_X_lst = [train_X_dict[pid][filter_trials_dict[pid][direc]] for pid in train_pids]\n",
    "#             test_Y_lst = [train_Y_dict[pid][filter_trials_dict[pid][direc]] for pid in train_pids]\n",
    "          \n",
    "#             proj_lst = []\n",
    "#             for pid_idx, pid in enumerate(train_pids):\n",
    "#                 proj = (test_X_lst[pid_idx].squeeze().numpy().transpose(0,-1,1) @ svd_U[pid_idx]) * S_mul_VT[pid_idx].T\n",
    "#                 proj_lst.append(proj.mean(0))\n",
    "#             all_svd_V.update({direc: np.array(proj_lst)})\n",
    "\n",
    "#         plt.figure()\n",
    "#         for direc in [\"L\", \"R\"]:\n",
    "#             plt.plot(np.abs(np.array(all_svd_V[direc])).mean(0)[:,0], label=direc)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "#         # trials with diff contrasts\n",
    "#         for level in [-1, -.25, -.125, -.0625, .0625, .125, .25, 1.]:\n",
    "#         # for level in [.0625, .125, .25, 1.]:\n",
    "#             try:\n",
    "#                 test_X_lst = [train_X_dict[pid][filter_trials_dict[pid][level]] for pid in train_pids]\n",
    "#                 test_Y_lst = [train_Y_dict[pid][filter_trials_dict[pid][level]] for pid in train_pids]\n",
    "#             except:\n",
    "#                 continue\n",
    "\n",
    "#             multi_task_rrm = Multi_Task_Reduced_Rank_Model(\n",
    "#                 n_tasks=len(train_pids),\n",
    "#                 n_units=n_units, \n",
    "#                 n_t_bins=T, \n",
    "#                 rank=R, \n",
    "#                 half_window_size=d,\n",
    "#                 # init_Us = init_Us,\n",
    "#                 # init_V = init_V\n",
    "#             )\n",
    "\n",
    "#             rrm, train_losses = train_multi_task(\n",
    "#                 model=multi_task_rrm,\n",
    "#                 train_dataset=(test_X_lst, test_Y_lst),\n",
    "#                 test_dataset=(test_X_lst, test_Y_lst),\n",
    "#                 loss_function=torch.nn.BCELoss(),\n",
    "#                 learning_rate=1e-3,\n",
    "#                 weight_decay=1e-1,\n",
    "#                 n_epochs=n_epochs,\n",
    "#             )\n",
    "\n",
    "#             test_U, test_V, _, _ = model_eval(\n",
    "#                 multi_task_rrm, \n",
    "#                 train_dataset=(test_X_lst, test_Y_lst),\n",
    "#                 test_dataset=(test_X_lst, test_Y_lst),\n",
    "#                 behavior=\"choice\"\n",
    "#             )\n",
    "\n",
    "#             Us, Vs = {}, {}\n",
    "#             for pid_idx, pid in enumerate(train_pids):\n",
    "#                 Us.update({pid: test_U[pid_idx]})\n",
    "#                 Vs.update({pid: test_V})\n",
    "\n",
    "#             svd_W, svd_U, svd_S, svd_VT, S_mul_VT, W_reduced = [], [], [], [], [], []\n",
    "#             for pid in train_pids:\n",
    "#                 W = np.array(Us[pid]) @ np.array(Vs[pid]).squeeze()\n",
    "#                 U, S, VT = svd(W)\n",
    "#                 svd_W.append(W)\n",
    "#                 svd_U.append(U[:, :R])\n",
    "#                 svd_S.append(S[:R])\n",
    "#                 svd_VT.append(VT[:R, :])\n",
    "#                 if len(S) == 1:\n",
    "#                     S_mul_VT.append(np.diag(S) @ VT[:1, :])\n",
    "#                 else:\n",
    "#                     S_mul_VT.append(np.diag(S[:R]) @ VT[:R, :])\n",
    "\n",
    "#             all_svd_V.update({level: S_mul_VT})\n",
    "\n",
    "#         # for level in [-1, -.25, -.125, -.0625, .0625, .125, .25, 1.]:\n",
    "#         for level in [.0625, .125, .25, 1.]:\n",
    "#             plt.figure()\n",
    "#             plt.plot(np.array(all_svd_V[-level]).mean(0).T[:,0], label=level)\n",
    "#             plt.plot(np.array(all_svd_V[level]).mean(0).T[:,0], label=level)\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "\n",
    "#         np.save(f\"../biorxiv_plots/results/{roi}_timescale.npy\", all_svd_V)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "        train_pids, n_units = [], []\n",
    "        train_X_dict, test_X_dict, train_Y_dict, test_Y_dict = {}, {}, {}, {}\n",
    "        for pid in loaded_pids:\n",
    "            X, Y = X_dict[pid], Y_dict[pid]\n",
    "            K, C, T = X.shape\n",
    "            if C < 10:\n",
    "                continue\n",
    "            train_pids.append(pid)\n",
    "            n_units.append(C)\n",
    "            X = sliding_window_over_trials(X, half_window_size=d)\n",
    "            Y = sliding_window_over_trials(Y, half_window_size=d)\n",
    "            X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "            train_X_dict.update({pid: [X[train] for train, _ in skf.split(X, Y)]})\n",
    "            test_X_dict.update({pid: [X[test] for _, test in skf.split(X, Y)]})\n",
    "            train_Y_dict.update({pid: [Y[train] for train, _ in skf.split(X, Y)]})\n",
    "            test_Y_dict.update({pid: [Y[test] for _, test in skf.split(X, Y)]})\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "        metrics_per_fold = []\n",
    "        for fold_idx in range(n_folds):\n",
    "\n",
    "            print(f\"{fold_idx+1}/{n_folds} folds remaining ...\")\n",
    "            train_X_lst = [train_X_dict[pid][fold_idx] for pid in train_pids]\n",
    "            test_X_lst = [test_X_dict[pid][fold_idx] for pid in train_pids]\n",
    "            train_Y_lst = [train_Y_dict[pid][fold_idx] for pid in train_pids]\n",
    "            test_Y_lst = [test_Y_dict[pid][fold_idx] for pid in train_pids]\n",
    "\n",
    "            multi_task_rrm = Multi_Task_Reduced_Rank_Model(\n",
    "                n_tasks=len(train_pids),\n",
    "                n_units=n_units, \n",
    "                n_t_bins=T, \n",
    "                rank=R, \n",
    "                half_window_size=d\n",
    "            )\n",
    "\n",
    "            rrm, train_losses = train_multi_task(\n",
    "                model=multi_task_rrm,\n",
    "                train_dataset=(train_X_lst, train_Y_lst),\n",
    "                test_dataset=(test_X_lst, test_Y_lst),\n",
    "                loss_function=torch.nn.BCELoss(),\n",
    "                learning_rate=1e-3,\n",
    "                weight_decay=1e-1,\n",
    "                n_epochs=n_epochs,\n",
    "            )\n",
    "\n",
    "            _, _, rrm_metrics, _ = model_eval(\n",
    "                multi_task_rrm, \n",
    "                train_dataset=(train_X_lst, train_Y_lst),\n",
    "                test_dataset=(test_X_lst, test_Y_lst),\n",
    "                behavior=\"choice\"\n",
    "            )\n",
    "            \n",
    "            frm_metrics = []\n",
    "            for pid_idx, pid in enumerate(train_pids):\n",
    "                clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1, 1e2, 1e3]).fit(\n",
    "                    train_X_lst[pid_idx].squeeze().numpy().reshape((len(train_X_lst[pid_idx]), -1)), \n",
    "                    train_Y_lst[pid_idx].numpy()\n",
    "                )\n",
    "                test_pred = clf.predict(\n",
    "                    test_X_lst[pid_idx].squeeze().numpy().reshape((len(test_X_lst[pid_idx]), -1))\n",
    "                )\n",
    "                frm_metrics.append(\n",
    "                    [accuracy_score(test_Y_lst[pid_idx].numpy(), test_pred), \n",
    "                     roc_auc_score(test_Y_lst[pid_idx].numpy(), test_pred)]\n",
    "                )\n",
    "            \n",
    "            metrics_per_fold.append(np.c_[rrm_metrics, frm_metrics])\n",
    "\n",
    "        metrics_dict = {}\n",
    "        for pid_idx, pid in enumerate(train_pids):\n",
    "            metrics_dict.update({pid: np.mean(metrics_per_fold, 0)[pid_idx]})\n",
    "        np.save(f\"../biorxiv_plots/results/{roi}_metrics.npy\", metrics_dict)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"time spent: {end_time - start_time: .3f} seconds\")\n",
    "        \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b47b100-1a67-4c13-b9fa-4121a86ff6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
