{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34c99679-0bcd-45fa-a454-6079471ea357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import date\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from one.api import ONE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchmetrics import AUROC\n",
    "import lightning as L \n",
    "from lightning.pytorch.utilities import CombinedLoader\n",
    "\n",
    "from side_info_decoding.utils import (\n",
    "    set_seed, \n",
    "    load_data_from_pids, \n",
    "    sliding_window_over_trials\n",
    ")\n",
    "\n",
    "seed = 666\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731e7323-ee92-4c6f-b6b1-363cba7be0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "regions = [\"LP\", \"GRN\"]\n",
    "n_sess = 2\n",
    "out_path = Path(\"/mnt/3TB/yizi/cached_ibl_data\")\n",
    "one = ONE(base_url=\"https://openalyx.internationalbrainlab.org\", mode='remote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a0ca831b-02cb-4c2f-a0d6-def21b5e8f46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Downloading data in region LP ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizi/anaconda3/envs/point_cloud/lib/python3.9/site-packages/ibllib/atlas/atlas.py:13: DeprecationWarning: ibllib.atlas.atlas.AllenAtlas is deprecated. Use iblatlas.atlas.AllenAtlas instead\n",
      "  warnings.warn(warning_text, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling data from ibl database ..\n",
      "eid: ebce500b-c530-47de-8cb1-963c552703ea\n",
      "pid: 8c732bf2-639d-496c-bf82-464bc9c2d54b\n",
      "number of trials found: 470\n",
      "found 470 trials from 13.74 to 5761.52 sec.\n",
      "found 139 Kilosort units in region lp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute spike count: 100%|███████████████████| 470/470 [00:01<00:00, 346.16it/s]\n",
      "/home/yizi/anaconda3/envs/point_cloud/lib/python3.9/site-packages/ibllib/atlas/atlas.py:13: DeprecationWarning: ibllib.atlas.atlas.AllenAtlas is deprecated. Use iblatlas.atlas.AllenAtlas instead\n",
      "  warnings.warn(warning_text, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling data from ibl database ..\n",
      "eid: 15b69921-d471-4ded-8814-2adad954bcd8\n",
      "pid: 7a620688-66cb-44d3-b79b-ccac1c8ba23e\n",
      "number of trials found: 715\n",
      "found 715 trials from 28.03 to 3547.82 sec.\n",
      "found 47 Kilosort units in region lp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute spike count: 100%|██████████████████| 715/715 [00:00<00:00, 2892.10it/s]\n",
      "/home/yizi/anaconda3/envs/point_cloud/lib/python3.9/site-packages/ibllib/atlas/atlas.py:13: DeprecationWarning: ibllib.atlas.atlas.AllenAtlas is deprecated. Use iblatlas.atlas.AllenAtlas instead\n",
      "  warnings.warn(warning_text, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling data from ibl database ..\n",
      "eid: ebce500b-c530-47de-8cb1-963c552703ea\n",
      "pid: 8c732bf2-639d-496c-bf82-464bc9c2d54b\n",
      "number of trials found: 470\n",
      "found 470 trials from 13.74 to 5761.52 sec.\n",
      "found 34 good units in region lp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute spike count: 100%|██████████████████| 470/470 [00:00<00:00, 1682.19it/s]\n",
      "/home/yizi/anaconda3/envs/point_cloud/lib/python3.9/site-packages/ibllib/atlas/atlas.py:13: DeprecationWarning: ibllib.atlas.atlas.AllenAtlas is deprecated. Use iblatlas.atlas.AllenAtlas instead\n",
      "  warnings.warn(warning_text, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling data from ibl database ..\n",
      "eid: 15b69921-d471-4ded-8814-2adad954bcd8\n",
      "pid: 7a620688-66cb-44d3-b79b-ccac1c8ba23e\n",
      "number of trials found: 715\n",
      "found 715 trials from 28.03 to 3547.82 sec.\n",
      "found 0 good units in region lp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute spike count: 100%|████████████████| 715/715 [00:00<00:00, 108096.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Downloaded 2 PIDs in region LP ..\n",
      "=================\n",
      "Successfully cached all data!\n",
      "=================\n",
      "Downloading data in region GRN ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/yizi/anaconda3/envs/point_cloud/lib/python3.9/site-packages/ibllib/atlas/atlas.py:13: DeprecationWarning: ibllib.atlas.atlas.AllenAtlas is deprecated. Use iblatlas.atlas.AllenAtlas instead\n",
      "  warnings.warn(warning_text, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling data from ibl database ..\n",
      "eid: c958919c-2e75-435d-845d-5b62190b520e\n",
      "pid: cc72fdb7-92e8-47e6-9cea-94f27c0da2d8\n",
      "number of trials found: 705\n",
      "found 705 trials from 79.14 to 3939.10 sec.\n",
      "found 261 Kilosort units in region grn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute spike count: 100%|███████████████████| 705/705 [00:06<00:00, 104.92it/s]\n",
      "/home/yizi/anaconda3/envs/point_cloud/lib/python3.9/site-packages/ibllib/atlas/atlas.py:13: DeprecationWarning: ibllib.atlas.atlas.AllenAtlas is deprecated. Use iblatlas.atlas.AllenAtlas instead\n",
      "  warnings.warn(warning_text, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling data from ibl database ..\n",
      "eid: 32d27583-56aa-4510-bc03-669036edad20\n",
      "pid: 2e720cee-05cc-440e-a24b-13794b1ac01d\n",
      "number of trials found: 682\n",
      "found 682 trials from 28.94 to 3431.07 sec.\n",
      "found 81 Kilosort units in region grn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute spike count: 100%|███████████████████| 682/682 [00:02<00:00, 294.69it/s]\n",
      "/home/yizi/anaconda3/envs/point_cloud/lib/python3.9/site-packages/ibllib/atlas/atlas.py:13: DeprecationWarning: ibllib.atlas.atlas.AllenAtlas is deprecated. Use iblatlas.atlas.AllenAtlas instead\n",
      "  warnings.warn(warning_text, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling data from ibl database ..\n",
      "eid: c958919c-2e75-435d-845d-5b62190b520e\n",
      "pid: cc72fdb7-92e8-47e6-9cea-94f27c0da2d8\n",
      "number of trials found: 705\n",
      "found 705 trials from 79.14 to 3939.10 sec.\n",
      "found 12 good units in region grn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute spike count: 100%|██████████████████| 705/705 [00:00<00:00, 1527.49it/s]\n",
      "/home/yizi/anaconda3/envs/point_cloud/lib/python3.9/site-packages/ibllib/atlas/atlas.py:13: DeprecationWarning: ibllib.atlas.atlas.AllenAtlas is deprecated. Use iblatlas.atlas.AllenAtlas instead\n",
      "  warnings.warn(warning_text, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling data from ibl database ..\n",
      "eid: 32d27583-56aa-4510-bc03-669036edad20\n",
      "pid: 2e720cee-05cc-440e-a24b-13794b1ac01d\n",
      "number of trials found: 682\n",
      "found 682 trials from 28.94 to 3431.07 sec.\n",
      "found 6 good units in region grn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute spike count: 100%|██████████████████| 682/682 [00:00<00:00, 1514.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Downloaded 2 PIDs in region GRN ..\n",
      "=================\n",
      "Successfully cached all data!\n"
     ]
    }
   ],
   "source": [
    "# download and cache data\n",
    "\n",
    "for roi_idx, roi in enumerate(regions):\n",
    "    \n",
    "    print(\"=================\")\n",
    "    print(f\"Downloading data in region {roi} ..\")\n",
    "    \n",
    "    pids = one.search_insertions(atlas_acronym=[roi], query_type='remote')\n",
    "    pids = list(pids)[:n_sess]\n",
    "    \n",
    "    # load choice\n",
    "    neural_dict, choice_dict = load_data_from_pids(\n",
    "        pids,\n",
    "        brain_region=roi.lower(),\n",
    "        behavior=\"choice\",\n",
    "        data_type=\"all_ks\",\n",
    "        n_t_bins = 40,\n",
    "    )\n",
    "    available_pids = list(neural_dict.keys())\n",
    "    \n",
    "    # load contrast\n",
    "    _, contrast_dict = load_data_from_pids(\n",
    "        pids,\n",
    "        brain_region=roi.lower(),\n",
    "        behavior=\"contrast\",\n",
    "        data_type=\"good_ks\",\n",
    "        n_t_bins = 40,\n",
    "    )\n",
    "\n",
    "    print(\"=================\")\n",
    "    print(f\"Downloaded {len(available_pids)} PIDs in region {roi} ..\")\n",
    "    \n",
    "    for _, pid in enumerate(available_pids):\n",
    "        xs, ys = neural_dict[pid], choice_dict[pid]\n",
    "        n_trials, n_units, n_t_bins = xs.shape\n",
    "        if n_units < 5:\n",
    "            continue\n",
    "        xs = sliding_window_over_trials(xs, half_window_size=0).squeeze()\n",
    "        ys = sliding_window_over_trials(ys, half_window_size=0).squeeze()\n",
    "        xs, ys = torch.tensor(xs), torch.tensor(ys)\n",
    "        \n",
    "        contrast_dict[pid] = np.nan_to_num(contrast_dict[pid], 0)\n",
    "        contrast_dict[pid].T[0] *= -1\n",
    "        contrast = contrast_dict[pid].sum(1)\n",
    "        \n",
    "        contrast_mask_dict = {}\n",
    "        for lvl in np.unique(np.abs(contrast)):\n",
    "            contrast_mask_dict.update(\n",
    "                {lvl: np.argwhere(contrast == lvl).flatten()}\n",
    "            )\n",
    "            \n",
    "        path = out_path/roi\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            \n",
    "        data_dict = {}\n",
    "        data_dict.update({'contrast': contrast})\n",
    "        data_dict.update({'contrast_mask': contrast_mask_dict})\n",
    "        data_dict.update({'meta':\n",
    "            {\"n_trials\": n_trials, \"n_units\": n_units, \"n_t_bins\": n_t_bins}\n",
    "        })\n",
    "        xs_per_lvl, ys_per_lvl = {}, {}\n",
    "        xs_per_lvl.update({\"all\": xs})\n",
    "        ys_per_lvl.update({\"all\": ys})\n",
    "        for lvl in np.unique(np.abs(contrast)):\n",
    "            try:\n",
    "                xs_per_lvl.update({lvl: xs[contrast_mask_dict[lvl]]})\n",
    "                ys_per_lvl.update({lvl: ys[contrast_mask_dict[lvl]]})\n",
    "            except:\n",
    "                continue\n",
    "        data_dict.update({'neural_contrast': xs_per_lvl})\n",
    "        data_dict.update({'choice_contrast': ys_per_lvl})\n",
    "        np.save(path/f\"pid_{pid}.npy\", data_dict)\n",
    "        \n",
    "    print(\"=================\")\n",
    "    print(f\"Successfully cached all data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bc7962a-763a-45a1-be42-112b1649c327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    def __init__(self, dataset, roi_idx, pid_idx, **kargs):\n",
    "        self.xs, self.ys = dataset\n",
    "        self.n_trials, self.n_units, _ = self.xs.shape\n",
    "        self.roi_idx = roi_idx\n",
    "        self.pid_idx = pid_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_trials\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.xs[index], self.ys[index], self.roi_idx, self.pid_idx\n",
    "    \n",
    "def dataloader(datasets, roi_idxs, pid_idxs, batch_size=32): \n",
    "    loaders = []\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        sess_dataset = SessionDataset(dataset, roi_idxs[i], pid_idxs[i])\n",
    "        loaders.append(DataLoader(\n",
    "            sess_dataset, batch_size = batch_size\n",
    "        ))\n",
    "    return loaders\n",
    "\n",
    "class Hier_Reduced_Rank_Model(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_roi,\n",
    "        n_units, \n",
    "        n_t_bin, \n",
    "        rank_V,\n",
    "        rank_B\n",
    "    ):\n",
    "        super(Hier_Reduced_Rank_Model, self).__init__()\n",
    "        \n",
    "        self.n_roi = n_roi\n",
    "        self.n_sess = len(n_units)\n",
    "        self.n_units = n_units\n",
    "        self.n_t_bin = n_t_bin\n",
    "        self.rank_V = rank_V\n",
    "        self.rank_B = rank_B\n",
    "        \n",
    "        self.Us = nn.ParameterList(\n",
    "            [nn.Parameter(torch.randn(self.n_units[i], self.rank_V)) for i in range(self.n_sess)]\n",
    "        )\n",
    "        self.A = nn.Parameter(torch.randn(self.n_roi, self.rank_V, self.rank_B)) \n",
    "        self.B = nn.Parameter(\n",
    "            torch.randn(self.rank_B, self.n_t_bin)\n",
    "        ) \n",
    "        self.intercepts = nn.ParameterList(\n",
    "            [nn.Parameter(torch.randn(1,)) for i in range(self.n_sess)]\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, datasets):\n",
    "        pred_lst, gt_lst = [], []\n",
    "        for dataset in datasets:\n",
    "            X, Y, roi_idx, sess_idx = dataset\n",
    "            roi_idx = torch.unique(roi_idx)\n",
    "            sess_idx = torch.unique(sess_idx)\n",
    "            n_trials, n_units, n_t_bins = X.shape\n",
    "            self.Vs = torch.einsum(\"ijk,kt->ijt\", self.A, self.B)\n",
    "            Beta = torch.einsum(\"cr,rt->ct\", self.Us[sess_idx], self.Vs[roi_idx].squeeze())\n",
    "            out = torch.einsum(\"ct,kct->k\", Beta, X)\n",
    "            out += self.intercepts[sess_idx] * torch.ones(n_trials)\n",
    "            out = self.sigmoid(out)\n",
    "            pred_lst.append(out)\n",
    "            gt_lst.append(Y)\n",
    "        return pred_lst, gt_lst\n",
    "    \n",
    "class LitHierRRR(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        losses = 0\n",
    "        pred_lst, gt_lst = self.model(batch)\n",
    "        for i in range(len(batch)):\n",
    "            losses += nn.BCELoss()(pred_lst[i], gt_lst[i])\n",
    "        loss = losses / len(batch)\n",
    "        self.log(\"loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        accs, aucs = self._shared_eval_step(batch)\n",
    "        metrics = {\"val_acc\": np.mean(accs), \"val_auc\": np.mean(aucs)}\n",
    "        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        accs, aucs = self._shared_eval_step(batch)\n",
    "        for i in range(len(batch)):\n",
    "            print(f\"session {i} test_acc {accs[i]} test_auc {aucs[i]}\")\n",
    "\n",
    "    def _shared_eval_step(self, batch):\n",
    "        pred_lst, gt_lst = self.model(batch)\n",
    "        accs, aucs = [], []\n",
    "        for i in range(len(batch)):\n",
    "            auroc = AUROC(task=\"binary\")\n",
    "            acc = accuracy(pred_lst[i], gt_lst[i], task=\"binary\")\n",
    "            auc = auroc(pred_lst[i], gt_lst[i])\n",
    "            accs.append(acc)\n",
    "            aucs.append(auc)\n",
    "        return accs, aucs\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7d82d9ea-aadb-452c-aea5-ffcc3fc7b797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "regions = [\"LP\", \"GRN\"]\n",
    "n_sess = 2\n",
    "n_rank_V = 2\n",
    "n_rank_B = 5\n",
    "n_epochs = 1000\n",
    "in_path = Path(\"/mnt/3TB/yizi/cached_ibl_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39383070-bdcc-4836-aa4d-8ce52faeb5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/yizi/anaconda3/envs/point_cloud/lib/python3.9/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name  | Type                    | Params\n",
      "--------------------------------------------------\n",
      "0 | model | Hier_Reduced_Rank_Model | 1.1 K \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n",
      "/home/yizi/anaconda3/envs/point_cloud/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/yizi/anaconda3/envs/point_cloud/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Loading 2 PIDs in region LP:\n",
      "7a620688-66cb-44d3-b79b-ccac1c8ba23e\n",
      "8c732bf2-639d-496c-bf82-464bc9c2d54b\n",
      "=================\n",
      "Loading 2 PIDs in region GRN:\n",
      "cc72fdb7-92e8-47e6-9cea-94f27c0da2d8\n",
      "2e720cee-05cc-440e-a24b-13794b1ac01d\n",
      "Epoch 36:  50%|██████▌      | 2/4 [00:00<00:00, 63.81it/s, v_num=29, loss=26.60]"
     ]
    }
   ],
   "source": [
    "# prep data for fitting hier-RRR on all trials\n",
    "\n",
    "res_dict = {}\n",
    "for lvl in [\"all\", .0625, .125, .25, 1.]:\n",
    "\n",
    "    lst_datasets, lst_units, lst_regions, lst_sessions, lst_region_names, lst_pids = [], [], [], [], [], []\n",
    "\n",
    "    pid_idx = 0\n",
    "    for roi_idx, roi in enumerate(regions):\n",
    "\n",
    "        f_names = os.listdir(in_path/roi)\n",
    "        pids = [f_name.split(\"_\")[1].split(\".\")[0] for f_name in f_names]\n",
    "\n",
    "        print(\"=================\")\n",
    "        print(f\"Loading {len(pids)} PIDs in region {roi}:\")\n",
    "        for pid in pids:\n",
    "            print(pid)\n",
    "\n",
    "        data_dict = np.load(in_path/roi/f\"pid_{pid}.npy\", allow_pickle=True).item()\n",
    "\n",
    "        for _, pid in enumerate(pids):\n",
    "            xs = data_dict[\"neural_contrast\"][lvl]\n",
    "            ys = data_dict[\"choice_contrast\"][lvl]\n",
    "            lst_datasets.append((xs, ys))\n",
    "            lst_units.append(data_dict[\"meta\"][\"n_units\"])\n",
    "            lst_regions.append(roi_idx)\n",
    "            lst_region_names.append(roi)\n",
    "            lst_sessions.append(pid_idx)\n",
    "            lst_pids.append(pid)\n",
    "            pid_idx += 1 \n",
    "\n",
    "    train_loaders = dataloader(lst_datasets, lst_regions, lst_sessions, batch_size=128)\n",
    "    train_loaders = CombinedLoader(train_loaders, mode=\"min_size\")\n",
    "\n",
    "    hier_rrr = Hier_Reduced_Rank_Model(\n",
    "        n_roi = len(regions),\n",
    "        n_units = lst_units, \n",
    "        n_t_bin = data_dict[\"meta\"][\"n_t_bins\"], \n",
    "        rank_V = n_rank_V,\n",
    "        rank_B = n_rank_B\n",
    "    )\n",
    "\n",
    "    lit_hier_rrr = LitHierRRR(hier_rrr)\n",
    "    trainer = L.Trainer(max_epochs=n_epochs)\n",
    "    trainer.fit(model=lit_hier_rrr, \n",
    "                train_dataloaders=train_loaders)\n",
    "\n",
    "    Us = [hier_rrr.Us[pid_idx].detach().numpy() for pid_idx in lst_sessions]\n",
    "    Vs = hier_rrr.Vs.detach().numpy()\n",
    "\n",
    "    svd_Vs = []\n",
    "    for pid_idx in lst_sessions:\n",
    "        roi_idx = lst_regions[pid_idx]\n",
    "        W = Us[pid_idx] @ Vs[roi_idx]\n",
    "        U, S, V = svd(W)\n",
    "        svd_Vs.append(np.diag(S[:n_rank_V]) @ V[:n_rank_V, :])\n",
    "    svd_Vs = np.array(svd_Vs)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(regions), 1, figsize=(5, 2*len(regions)))\n",
    "    for i, roi_idx in enumerate(np.unique(lst_regions)):\n",
    "        mask = np.array(lst_regions) == roi_idx\n",
    "        axes[i].plot(np.abs(svd_Vs[mask].mean(0)[0]))\n",
    "        axes[i].set_title(f\"{regions[roi_idx]} (contrast = {lvl})\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    res_dict.update({lvl: {}})\n",
    "    res_dict[lvl].update({\"pid_idxs\": lst_sessions})\n",
    "    res_dict[lvl].update({\"regions_idxs\": lst_regions})\n",
    "    res_dict[lvl].update({\"region_names\": lst_region_names})\n",
    "    res_dict[lvl].update({\"pids\": lst_pids})\n",
    "    res_dict[lvl].update({\"svd_Vs\": svd_Vs})\n",
    "\n",
    "np.save(in_path/f\"res_{date.today()}.npy\", res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2276565-4c22-458f-b7dd-8df769f48971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep data for x-val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb1460-0976-4c7e-a638-831ac79ace90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87014030-074a-48ce-93ad-60050ea7e76f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
