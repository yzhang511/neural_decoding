"""Baseline models including single/multi-session decoders."""

import numpy as np
import torch
from lightning.pytorch import LightningModule
from lightning.pytorch.callbacks import ModelCheckpoint
from torchmetrics import R2Score, Accuracy
from torch.nn import functional as F

def tuple_type(strings):
    strings = strings.replace("(", "").replace(")", "")
    mapped_int = map(int, strings.split(","))
    return tuple(mapped_int)

# --------------------------
# Single-session decoders
# --------------------------

class BaselineDecoder(LightningModule):
    def __init__(self, config):
        """Backbone for single-session decoders: reduced-rank, MLP, LSTM.
        
        Args:
            config - model config that includes:
                n_units: number of neurons in a session.
                n_t_steps: number of time steps in a session.
                target: 'cls' - classification for discrete behavior; 'reg' - regression for continuous behavior.
                output_size: behavior dimension. 
        """
        super().__init__()
        self.n_units = config['n_units']
        self.n_t_steps = config['n_t_steps']
        self.learning_rate = config['optimizer']['lr']
        self.weight_decay = config['optimizer']['weight_decay']
        self.target = config['model']['target']
        self.output_size = config['model']['output_size']
        
        self.r2_score = R2Score(num_outputs=self.n_t_steps, multioutput='uniform_average')
        self.accuracy = Accuracy(task="multiclass", num_classes=self.output_size)

    def forward(self, x):
        pass

    def training_step(self, batch, batch_idx):
        x, y, _, _ = batch
        pred = self(x)
        if self.target == 'reg':
            loss = F.mse_loss(pred, y)
        elif self.target == 'clf':
            loss = torch.nn.CrossEntropyLoss()(pred, y)
        else:
            raise NotImplementedError
        self.log('loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def validation_step(self, batch, batch_idx, print_str="val"):
        x, y, _, _ = batch
        pred = self(x)
        if self.target == 'reg':
            loss = F.mse_loss(pred, y)
            self.r2_score(pred.flatten(), y.flatten())
            self.log(f"{print_str}_metric", self.r2_score, prog_bar=True, logger=True, sync_dist=True)
        elif self.target == 'clf':
            loss = torch.nn.CrossEntropyLoss()(pred, y)
            self.accuracy(F.softmax(pred, dim=1).argmax(1), y.argmax(1))
            self.log(f"{print_str}_metric", self.accuracy, prog_bar=True, logger=True, sync_dist=True)
        else:
            raise NotImplementedError
        self.log(f"{print_str}_loss", loss, prog_bar=True)
        return loss

    def test_step(self, batch, batch_idx):
        return self.validation_step(batch, batch_idx, print_str='test')

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.parameters(), lr = self.learning_rate, weight_decay = self.weight_decay
        )
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}

    
class ReducedRankDecoder(BaselineDecoder):
    def __init__(self, config):
        """Single-session reduced-rank decoder.

        Args:
            config - model config that includes:
                temporal_rank: rank (R) of U, V; constraint: R <= min(n_units, n_t_steps).

        Params:
            U: neural basis set (n_units, rank).
            V: temporal basis set (rank, n_t_steps, output_size).
            b: intercept term (output_size,).
        """
        super().__init__(config)
        self.temporal_rank = config['reduced_rank']['temporal_rank']
        self.U = torch.nn.Parameter(torch.randn(self.n_units, self.temporal_rank))
        self.V = torch.nn.Parameter(torch.randn(self.temporal_rank, self.n_t_steps, self.output_size))
        self.b = torch.nn.Parameter(torch.randn(self.output_size,))
        self.double()

    def forward(self, x):
        self.B = torch.einsum('nr,rtd->ntd', self.U, self.V)
        pred = torch.einsum('ntd,ktn->kd', self.B, x)
        pred += self.b
        return pred


class MLPDecoder(BaselineDecoder):
    def __init__(self, config):
        """Single-session MLP decoder.

        Args:
            config - model config that includes:
                hidden_size: depth and size of hidden layers, e.g., (128, 64) has 2 layers with size 128 and 64.
                drop_out: drop out ratio.
        """
        super().__init__(config)
        self.hidden_size = tuple_type(config['mlp']['mlp_hidden_size'])
        self.drop_out = config['mlp']['drop_out']
        
        self.input_layer = torch.nn.Linear(self.n_units, self.hidden_size[0])
        self.hidden_lower = torch.nn.ModuleList()
        for l in range(len(self.hidden_size)-1):
            self.hidden_lower.append(torch.nn.Linear(self.hidden_size[l], self.hidden_size[l+1]))
            self.hidden_lower.append(torch.nn.ReLU())
            self.hidden_lower.append(torch.nn.Dropout(self.drop_out))
        self.flat_layer = torch.nn.Linear(self.hidden_size[-1]*self.n_t_steps, self.hidden_size[0])

        self.hidden_upper = torch.nn.ModuleList()
        for l in range(len(self.hidden_size)-1):
            self.hidden_upper.append(torch.nn.Linear(self.hidden_size[l], self.hidden_size[l+1]))
            self.hidden_upper.append(torch.nn.ReLU())
            self.hidden_upper.append(torch.nn.Dropout(self.drop_out))
        self.output_layer = torch.nn.Linear(self.hidden_size[-1], self.output_size)
        self.double()

    def forward(self, x):
        x = self.input_layer(x)
        for layer in self.hidden_lower:
            x = layer(x)
        x = F.relu(self.flat_layer(x.flatten(start_dim=1)))
        
        for layer in self.hidden_upper:
            x = layer(x)
        pred = self.output_layer(x)
        return pred
    
    
class LSTMDecoder(BaselineDecoder):
    def __init__(self, config):
        """Single-session LSTM decoder.

        Args:
            config - model config that includes:
                lstm_hidden_size: size of LSTM hidden layers
                lstm_n_layers: depth of LSTM hidden layers
                mlp_hidden_size: depth and size of MLP hidden layers, e.g., (128, 64) has 2 layers with size 128 and 64.
                drop_out: drop out ratio.
        """
        super().__init__(config)
        self.lstm_hidden_size = config['lstm']['lstm_hidden_size']
        self.n_layers = config['lstm']['lstm_n_layers']
        self.hidden_size = tuple_type(config['lstm']['mlp_hidden_size'])
        self.drop_out = config['lstm']['drop_out']

        self.lstm = torch.nn.LSTM(
            input_size=self.n_units,
            hidden_size=self.lstm_hidden_size,
            num_layers=self.n_layers,
            dropout=self.drop_out,
            batch_first=True,
        )

        self.input_layer = torch.nn.Linear(self.lstm_hidden_size, self.hidden_size[0])
        self.hidden = torch.nn.ModuleList()
        for l in range(len(self.hidden_size)-1):
            self.hidden.append(torch.nn.Linear(self.hidden_size[l], self.hidden_size[l+1]))
            self.hidden.append(torch.nn.ReLU())
            self.hidden.append(torch.nn.Dropout(self.drop_out))
        self.output_layer = torch.nn.Linear(self.hidden_size[-1], self.output_size) 
        self.double()

    def forward(self, x):
        # lstm_out: (batch_size, seq_len, hidden_size)
        lstm_out, _ = self.lstm(x)
        x = F.relu(self.input_layer(lstm_out[:,-1]))
        for layer in self.hidden:
            x = layer(x)
        pred = self.output_layer(x)
        return pred


# --------------------------
# Multi-session decoders
# --------------------------

class BaselineMultiSessionDecoder(LightningModule):
    def __init__(self, config):
        """Backbone for multi-session decoders: multi-session / multi-region reduced-rank models.
        
        Args:
            config - model config that includes:
                n_sess: number of sessions.
                n_units: a list of number of neurons in each session.
                n_t_steps: number of time steps in each session.
                target: 'cls' - classification for discrete behavior; 'reg' - regression for continuous behavior.
                output_size: behavior dimension. 
        """
        super().__init__()
        self.n_sess = len(config['n_units'])
        self.n_units = config['n_units']
        self.n_t_steps = config['n_t_steps']
        self.target = config['model']['target']
        self.output_size = config['model']['output_size']
        self.learning_rate = config['optimizer']['lr']
        self.weight_decay = config['optimizer']['weight_decay']

        self.r2_score = R2Score(num_outputs=self.n_t_steps, multioutput='uniform_average')
        self.accuracy = Accuracy(task="multiclass", num_classes=self.output_size)

    def forward(self, x):
        pass

    def training_step(self, batch, batch_idx):
        loss = torch.zeros(len(batch))
        for idx, session in enumerate(batch):
            x, y, region, eid = session
            # NOTE:
            # each batch has len(batch) eids and regions but we only need one string for each entry
            # each batch consists of data from same session and region
            pred = self(x, eid[0], region[0])
            loss[idx] = torch.nn.MSELoss()(pred, y)
            if self.target == 'reg':
                loss[idx] = torch.nn.MSELoss()(pred, y)
            elif self.target == 'clf':
                loss[idx] = torch.nn.CrossEntropyLoss()(pred, y)
            else:
                raise NotImplementedError
        loss = torch.mean(loss)
        self.log('loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)
        return loss

    def validation_step(self, batch, batch_idx, print_str="val"):
        loss, metric = torch.zeros(len(batch)), torch.zeros(len(batch))
        for idx, session in enumerate(batch):
            x, y, region, eid = session
            # NOTE:
            # each batch has len(batch) eids and regions but we only need one string for each entry
            # each batch consists of data from same session and region
            pred = self(x, eid[0], region[0])
            if self.target == 'reg':
                loss[idx] = torch.nn.MSELoss()(pred, y)
                metric[idx] = self.r2_score(pred.flatten(), y.flatten())
            elif self.target == 'clf':
                loss = torch.nn.CrossEntropyLoss()(pred, y)
                metric[idx] = self.accuracy(F.softmax(pred, dim=1).argmax(1), y.argmax(1))
            else:
                raise NotImplementedError
        loss, metric = torch.mean(loss), torch.mean(metric)
        self.log(f"{print_str}_loss", loss, prog_bar=True, logger=True, sync_dist=True)
        self.log(f"{print_str}_metric", metric, prog_bar=True, logger=True, sync_dist=True)
        return loss

    def test_step(self, batch, batch_idx):
        return self.validation_step(batch, batch_idx, print_str='test')

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.parameters(), lr = self.learning_rate, weight_decay = self.weight_decay
        )
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}


class MultiSessionReducedRankDecoder(BaselineMultiSessionDecoder):
    def __init__(self, config):
        """Multi-session reduced-rank decoder.

        Args:
            config - model config that includes:
                temporal_rank: rank (R) of U, V; constraint: R <= min(n_units, n_t_steps).
                eid_to_indx: a dict converting eid to session index; used to track session-specific params.
                n_units: a list of number of neurons in each session.

        Params:
            Us: a list of neural basis sets, e.g., [(N1, R), (N2, R), ...], N1 is n_units in session 1.
            V: temporal basis set (rank, n_t_steps, output_size).
            bs: a list of intercept terms, e.g., [(output_size,), (output_size,), ...].
        """
        super().__init__(config)
        self.temporal_rank = config['temporal_rank']
        self.eid_to_indx = config['eid_to_indx']
        self.n_units = config['n_units']

        self.Us = torch.nn.ParameterList(
            [torch.nn.Parameter(torch.randn(n_units, self.temporal_rank)) for n_units in self.n_units]
        )
        self.V = torch.nn.Parameter(torch.randn(self.temporal_rank, self.n_t_steps, self.output_size))
        self.bs = torch.nn.ParameterList(
            [torch.nn.Parameter(torch.randn(self.output_size,)) for _ in self.n_units]
        )
        self.double()

    def forward(self, x, eid, region):
        idx = self.eid_to_indx[eid]
        B = torch.einsum('nr,rtd->ntd', self.Us[idx], self.V)
        pred = torch.einsum('ntd,ktn->kd', B, x)
        pred += self.bs[idx]
        return pred
    

    
class MultiRegionReducedRankDecoder(BaselineMultiSessionDecoder):
    def __init__(self, config):
        """Multi-region reduced-rank decoder.

        Args:
            config - model config that includes:
                temporal_rank: rank (R) of U, V; constraint: R <= min(n_units, n_t_steps).
                global_basis_rank: rank (L) of global temporal basis set (B).
                n_regions: number of input brain regions.
                region_to_indx: a dict converting region name to index; used to track region-specific params.
                eid_region_to_indx: a dict indexing eid-region combination; used to track params per session-region.

        Params:
            Us: a list of neural basis sets, e.g., [(N1, R), (N2, R), ...], N1 is n_units in session-region 1.
            A: region-specific basis set (n_regions, R, L).
            B: global temporal basis set (L, n_t_steps, output_size).
            bs: a list of intercept terms, e.g., [(output_size,), (output_size,), ...].
        """
        super().__init__(config)
        self.temporal_rank = config['temporal_rank']
        self.global_basis_rank = config['global_basis_rank']
        self.n_regions = config['n_regions']
        self.region_to_indx = config['region_to_indx']
        self.eid_region_to_indx = config['eid_region_to_indx']

        self.Us = torch.nn.ParameterList(
            [torch.nn.Parameter(torch.randn(n_units, self.temporal_rank)) for n_units in self.n_units]
        )
        self.A = torch.nn.Parameter(torch.randn(self.n_regions, self.temporal_rank, self.global_basis_rank))
        self.B = torch.nn.Parameter(torch.randn(self.global_basis_rank, self.n_t_steps, self.output_size))     
        self.bs = torch.nn.ParameterList(
            [torch.nn.Parameter(torch.randn(self.output_size,)) for _ in self.n_units]
        )
        self.double()
        
    def forward(self, x, eid, region):
        idx = self.eid_region_to_indx[eid][region]
        region_idx = self.region_to_indx[region]
        self.Vs = torch.einsum("jrl,ltp->jrtp", self.A, self.B)
        U, V = self.Us[idx], self.Vs[region_idx].squeeze()
        W = torch.einsum("nr,rtp->ntp", U, V)        
        pred = torch.einsum('ntp,ktn->kp', W, x)
        pred += self.bs[idx]
        return pred
        
    