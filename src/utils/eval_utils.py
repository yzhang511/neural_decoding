"""Evaluation for single/multi-session decoders."""

import numpy as np
import torch
from torch.nn.functional import softmax
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score, accuracy_score

# --------------------------
# Single-session evaluation
# --------------------------

def eval_model(
    train, test, model, 
    target='reg', 
    model_class='reduced_rank', 
    training_type='single-sess', 
):
    """Model evaluation backbone.
        
    Args:
        train: training data (torch.utils.data.Dataset). 
        test: test data (torch.utils.data.Dataset). 
        model: pretrained PyTorch model.
        target: 
            'cls': classification for discrete behavior.
            'reg': regression for continuous behavior.
        model_class: options = ['linear', 'reduced_rank', 'mlp', 'lstm'].
        training_type: options = ['single-sess', 'multi-sess'].
    """
    # Load data
    train_x, train_y = [], []
    for (x, y, region, eid) in train:
        train_x.append(x.cpu())
        train_y.append(y.cpu())

    test_x, test_y = [], []
    for (x, y, region, eid) in test:
        test_x.append(x.cpu())
        test_y.append(y.cpu())
        
    if training_type == 'multi-sess':
        train_x, train_y = torch.vstack(train_x), torch.vstack(train_y)
        test_x, test_y = torch.vstack(test_x), np.vstack(test_y)
    elif training_type == 'single-sess':
        train_x, train_y = torch.stack(train_x), torch.stack(train_y)
        test_x, test_y = torch.stack(test_x), np.stack(test_y)
    else:
        raise NotImplementedError

    # Model prediction
    if model_class == 'reduced_rank':
        if training_type == 'multi-sess':
            test_pred = model(test_x, eid[0], region[0])
        else:
            test_pred = model(test_x)
        if target == 'clf':
            test_pred = softmax(test_pred, dim=1).detach().numpy().argmax(1)
        elif target == 'reg':
            test_pred = test_pred.detach().numpy()
    elif model_class == 'linear':
        train_x, test_x = train_x.numpy(), test_x.numpy()
        if target == 'clf':
            model.fit(train_x.reshape((train_x.shape[0], -1)), train_y.argmax(1))
        elif target == 'reg':
            model.fit(train_x.reshape((train_x.shape[0], -1)), train_y)
        test_pred = model.predict(test_x.reshape((test_x.shape[0], -1)))
    elif model_class in ['mlp', 'lstm']:
        test_pred = model(test_x)
        if target == 'clf':
            test_pred = softmax(test_pred, dim=1).detach().numpy().argmax(1)
        elif target == 'reg':
            test_pred = test_pred.detach().numpy()
    else:
        raise NotImplementedError

    # Evaluation
    if target == 'reg':
        metric = r2_score(test_y.flatten(), test_pred.flatten())
    elif target == 'clf':
        metric = accuracy_score(test_y.argmax(1), test_pred)
    else:
        raise NotImplementedError
        
    return metric, test_pred, test_y


# --------------------------
# Multi-session evaluation
# --------------------------

def eval_multi_session_model(
    train_lst, 
    test_lst, 
    model, 
    target='reg',
    model_class='reduced_rank', 
):
    """Multi-session model evaluation.
        
    Args:
        train_lst: a list of training data (torch.utils.data.Dataset) from different sessions. 
        test_lst: a list of test data (torch.utils.data.Dataset) from different sessions. 
        model: pretrained PyTorch model.
        target: 
            'cls': classification for discrete behavior.
            'reg': regression for continuous behavior.
        model_class: options = ['reduced_rank'].
    """
    assert model_class=='reduced_rank', 'Other models do not support multi-session training yet. '
    
    metric_lst, test_pred_lst, test_y_lst = [], [], []
    for idx, (train, test) in enumerate(zip(train_lst, test_lst)):
        metric, test_pred, test_y = eval_model(
            train, test, model, target=target, 
            model_class=model_class, training_type='multi-sess',
        )
        metric_lst.append(metric)
        test_pred_lst.append(test_pred)
        test_y_lst.append(test_y)
    return metric_lst, test_pred_lst, test_y_lst


# --------------------------
# Multi-region evaluation
# --------------------------

def eval_multi_region_model(
    train_lst, 
    test_lst, 
    model, 
    target='reg',
    model_class='reduced_rank', 
    all_regions=None,
    configs=None,
):
    """Multi-region model evaluation.
        
    Args:
        train_lst: a list of training data (torch.utils.data.Dataset) from different sessions. 
        test_lst: a list of test data (torch.utils.data.Dataset) from different sessions. 
        model: pretrained PyTorch model.
        target: 
            'cls': classification for discrete behavior.
            'reg': regression for continuous behavior.
        model_class: options = ['reduced_rank'].
        all_regions: a list of all available input regions.
        configs: a list of model configs for each session-region combination. 
    """
    assert model_class=='reduced_rank', 'Other models do not support multi-region training yet. '
    
    metric_dict, test_pred_dict, test_y_dict = {}, {}, {}
    for region in all_regions:
        metric_dict[region], test_pred_dict[region], test_y_dict[region] = {}, {}, {}
        
    for idx, (train, test) in enumerate(zip(train_lst, test_lst)):
        eid = configs[idx]['eid']
        region = configs[idx]['region']
        metric, test_pred, test_y = eval_model(
            train, test, model, target=target, 
            model_class=model_class, training_type='multi-sess',
        )
        metric_dict[region][eid] = metric
        test_pred_dict[region][eid] = test_pred
        test_y_dict[region][eid] = test_y
    return metric_dict, test_pred_dict, test_y_dict
    
