"Code for different versions of Beta mixture model + hidden Markov model (BMM-HMM)."
import numpy as np
import math
from math import log
from scipy import stats
from scipy.special import logsumexp 
from scipy.optimize import minimize
import pymc3 as pm

def beta_comp1_cons(params):
    a0, _, b0, _ = params
    return a0 - b0

def beta_comp2_cons(params):
    _, a1, _, b1 = params
    return b1 - a1

constraints = [
    {'type': 'ineq', 'fun': beta_comp1_cons},
    {'type': 'ineq', 'fun': beta_comp2_cons}
]

class BetaProcess(pm.Continuous):
    
    def __init__(self, y=None, alpha=None, beta=None, *args, **kwargs):
        
        super(BetaProcess, self).__init__(*args, **kwargs)
        
        self.y = y
        self.alpha = alpha
        self.beta = beta

    def logp(self, x):
        alpha, beta = self.alpha[self.y], self.beta[self.y]
        llike = pm.Beta.dist(alpha, beta).logp_sum(x)
        return llike


def posterior_inference(model, d):
    
    K = len(d)
    alpha = model.forward(d)
    beta = model.backward(d)

    gammas = []
    for k in range(K):
        gammas.append(model.gamma(k, d, alpha, beta))

    y_prob = []
    for k in range(K):
        unnorm_prob = 0
        for h in model.states:
            unnorm_prob += np.exp(model.gamma_partial(k, d, gammas[k])[h][0])
        y_prob.append(unnorm_prob)

    y_prob = np.array(y_prob)
    y_pred = [1 if prob > .5 else 0 for prob in y_prob]
    
    return y_pred, y_prob



class HMM(object):
    """
    This code is taken from ... (add GitHub link).
    """
    def __init__(self, init_probs):
        self.pi = init_probs[0]
        self.a = init_probs[1]
        self.b = init_probs[2]

    @staticmethod
    def log_sum_exp(seq):
        if abs(min(seq)) > abs(max(seq)):
            a = min(seq)
        else:
            a = max(seq)

        total = 0
        for x in seq:
            try:
                total += math.exp(x - a)
            except:
                total += math.exp(700)
        return a + math.log(total)

    def forward(self, sequence):
        N = len(self.pi)
        alpha = []

        d1 = {}
        for i in [0, 1, 2]:
            d1[i] = self.pi[i] + self.b[i][sequence[0]]
        alpha.append(d1)

        for t in range(1, len(sequence)):
            d = {}
            o = sequence[t]

            for j in [0, 1, 2]:
                sum_seq = []
                for i in [0, 1, 2]:
                    sum_seq.append(alpha[-1][i] + self.a[i][j])

                d[j] = self.log_sum_exp(sum_seq) + self.b[j][o]

            alpha.append(d)

        return alpha

    def forward_probability(self, alpha):
        return self.log_sum_exp(alpha[-1].values())

    def backward(self, sequence):
        N = len(self.pi)
        T = len(sequence)
        beta = []

        dT = {}
        for i in [0, 1, 2]:
            dT[i] = 0
        beta.append(dT)

        for t in range(T - 2, -1, -1):
            d = {}
            o = sequence[t + 1]

            for i in [0, 1, 2]:
                sum_seq = []
                for j in [0, 1, 2]:
                    sum_seq.append(self.a[i][j] + self.b[j][o] + beta[-1][j])
                d[i] = self.log_sum_exp(sum_seq)

            beta.append(d)
        beta.reverse()
        return beta

    def backward_probability(self, beta, sequence):
        N = len(self.pi)
        sum_seq = []

        for i in [0, 1, 2]:
            sum_seq.append(self.pi[i] + self.b[i][sequence[0]] + beta[0][i])
        return self.log_sum_exp(sum_seq)

    def forward_backward(self, sequence):
        N = len(self.pi)
        alpha = self.forward(sequence)
        beta = self.backward(sequence)
        T = len(sequence)

        xis = []
        for t in range(T - 1):
            xis.append(self.xi_matrix(t, sequence, alpha, beta))
        gammas = []
        for t in range(T):
            gammas.append(self.gamma(t, sequence, alpha, beta, xis))

        pi_hat = gammas[0]
        a_hat = {}
        b_hat = {}
        for i in [0, 1, 2]:
            a_hat[i] = {}
            b_hat[i] = {}

            sum_seq = []
            for t in range(T - 1):
                sum_seq.append(gammas[t][i])
            a_hat_denom = self.log_sum_exp(sum_seq)

            for j in [0, 1, 2]:
                sum_seq = []
                for t in range(T - 1):
                    sum_seq.append(xis[t][i][j])
                a_hat_num = self.log_sum_exp(sum_seq)
                a_hat[i][j] = a_hat_num - a_hat_denom

            sum_seq = []
            for t in range(T):
                sum_seq.append(gammas[t][i])
            b_hat_denom = self.log_sum_exp(sum_seq)
            for k in self.b[i]:
                sum_seq = []
                for t in range(T):
                    o = sequence[t]
                    if o == k:
                        sum_seq.append(gammas[t][i])
                b_hat_num = self.log_sum_exp(sum_seq)
                b_hat[i][k] = b_hat_num - b_hat_denom

        return (pi_hat, a_hat, b_hat)

    def gamma(self, t, sequence, alpha, beta, xis):
        N = len(self.pi)
        gamma = {}
        if t < len(sequence) - 1:
            xi = xis[t]
            for i in [0, 1, 2]:
                sum_seq = []
                for j in [0, 1, 2]:
                    sum_seq.append(xi[i][j])
                gamma[i] = self.log_sum_exp(sum_seq)
        else:
            sum_seq = []
            for i in [0, 1, 2]:
                gamma[i] = alpha[t][i] + beta[t][i]
                sum_seq.append(gamma[i])

            denom = self.log_sum_exp(sum_seq)

            for i in [0, 1, 2]:
                gamma[i] -= denom

        return gamma

    def xi_matrix(self, t, sequence, alpha, beta):
        N = len(self.pi)
        o = sequence[t+1]

        xi = {}

        sum_seq = []

        for i in [0, 1, 2]:
            xi[i] = {}
            for j in [0, 1, 2]:
                num = alpha[t][i] + self.a[i][j] \
                      + self.b[j][o] + beta[t + 1][j]
                sum_seq.append(num)
                xi[i][j] = num

        denom = self.log_sum_exp(sum_seq)

        for i in [0, 1, 2]:
            for j in [0, 1, 2]:
                xi[i][j] -= denom

        return xi

    def update(self, sequence, cutoff_value):
        increase = cutoff_value + 1
        while (increase > cutoff_value):
            before = self.forward_probability(self.forward(sequence))
            new_p = self.forward_backward(sequence)
            self.pi = new_p[0]
            self.a = new_p[1]
            self.b = new_p[2]
            after = self.forward_probability(self.forward(sequence))
            increase = after - before

            

class BMM_HMM(object):
    def __init__(
        self, 
        d, 
        init_pi, 
        init_a, 
        init_phi, 
        init_beta_a, 
        init_beta_b,
        tol=1e-1
    ):
        self.d = d
        self.pi = init_pi
        self.states = list(init_pi.keys())
        self.a = init_a
        self.phi = init_phi
        self.beta_a = init_beta_a
        self.beta_b = init_beta_b
        self.tol = tol
        
    def change_beta_parametrization(self, beta_a, beta_b):
        mu = beta_a / (beta_a + beta_b)
        var = beta_a * beta_b / (pow(beta_a+beta_b, 2)*(beta_a+beta_b+1))
        return mu, var
    
    def equilibrium_probs(self, a):
        A = np.exp([list(a[0].values()), list(a[1].values()), list(a[2].values())])
        pi = A.copy()
        for _ in range(100):
            pi = np.matmul(pi, A)
        pi = pi[0]
        return np.log(pi + 1e-8)
    
    def bmm_neg_loglike(self, params):
        K = len(self.d)
        beta_a, beta_b = params[:2], params[2:]    

        lpdf = np.zeros((K, 2))
        lpdf[:,0] = stats.beta.logpdf(self.d, beta_a[0], beta_b[0])
        lpdf[:,1] = stats.beta.logpdf(self.d, beta_a[1], beta_b[1])

        tau = [np.exp(list(self.tau(k, self.d, self.equi_pi).values())) for k in range(K)]
        state_ll = [list(self.phi[h].values()) + self.equi_pi[h] for h in self.states]

        nll = [-1. * (logsumexp(state_ll + lpdf[k], 0) * tau[k]).sum() for k in range(K)]
        return np.sum(nll)
        
    def emission_probs(self, h, yk, dk, marginalize=False):
        
        if marginalize:
            b_h = []
            b_h.append(self.phi[h][0] + stats.beta.logpdf(dk, self.beta_a[0], self.beta_b[0]))
            b_h.append(self.phi[h][1] + stats.beta.logpdf(dk, self.beta_a[1], self.beta_b[1]))
            b_h = logsumexp(b_h)
        else:
            b_h = self.phi[h][yk] + stats.beta.logpdf(dk, self.beta_a[yk], self.beta_b[yk])
        
        return b_h

    def forward(self, d):
        alpha = []

        dict_h = {}
        for h in self.states:
            dict_h[h] = self.pi[h] + self.emission_probs(h, None, d[0], marginalize=True) 
        alpha.append(dict_h)

        for k in range(1, len(d)):
            dict_h = {}
            dk = d[k]

            for h in self.states:
                sum_seq = []
                for m in self.states:
                    sum_seq.append(alpha[-1][m] + self.a[m][h])
                dict_h[h] = logsumexp(sum_seq) + self.emission_probs(h, None, dk, marginalize=True)
            alpha.append(dict_h)

        return alpha

    def forward_probability(self, alpha):
        return logsumexp(list(alpha[-1].values()))

    def backward(self, d):
        K = len(d)
        beta = []

        dict_h = {}
        for h in self.states:
            dict_h[h] = 0
        beta.append(dict_h)

        for k in range(K - 2, -1, -1):
            dict_h = {}
            dk = d[k + 1]

            for h in self.states:
                sum_seq = []
                for m in self.states:
                    sum_seq.append(self.a[h][m] + self.emission_probs(m, None, dk, marginalize=True) + beta[-1][m])
                dict_h[h] = logsumexp(sum_seq)
            beta.append(dict_h)
            
        beta.reverse()
        return beta

    def backward_probability(self, beta, d):
        sum_seq = []
        for h in self.states:
            sum_seq.append(self.pi[h] + self.emission_probs(h, None, d[0], marginalize=True) + beta[0][h])
        return logsumexp(sum_seq)

    def forward_backward(self, d):
        alpha = self.forward(d)
        beta = self.backward(d)
        K = len(d)

        xis = []
        for k in range(K - 1):
            xis.append(self.xi_matrix(k, d, alpha, beta))
            
        gammas = []
        for k in range(K):
            gammas.append(self.gamma(k, d, alpha, beta))
            
        gammas_partial = []
        for k in range(K):
            gammas_partial.append(self.gamma_partial(k, d, gammas[k]))

        pi_hat = gammas[0]
        
        a_hat = {}
        for h in self.states:
            a_hat[h] = {}
            sum_seq = []
            for k in range(K - 1):
                sum_seq.append(gammas[k][h])
            a_hat_denom = logsumexp(sum_seq)

            for m in self.states:
                sum_seq = []
                for k in range(K - 1):
                    sum_seq.append(xis[k][h][m])
                a_hat_num = logsumexp(sum_seq)
                a_hat[h][m] = a_hat_num - a_hat_denom
         
        phi_hat = {}
        for h in self.states:
            phi_hat[h] = {}
            sum_seq = []
            for k in range(K):
                sum_seq.append(gammas[k][h])
            phi_hat_denom = logsumexp(sum_seq)
            
            for yk in [0, 1]:
                sum_seq = []
                for k in range(K):
                    sum_seq.append(gammas_partial[k][h][yk])
                phi_hat_num = logsumexp(sum_seq)
                phi_hat[h][yk] = phi_hat_num - phi_hat_denom
        
        init_params = np.hstack([self.beta_a, self.beta_b])
        self.equi_pi = self.equilibrium_probs(a=a_hat)
        res = minimize(self.bmm_neg_loglike, init_params, constraints=constraints, tol=self.tol,
                       options={'disp': False})
        print("BMM Convergence Achieved:", res.success)
        beta_a_hat = [res.x[0], res.x[1]]
        beta_b_hat = [res.x[2], res.x[3]]
            
        return (pi_hat, a_hat, phi_hat, beta_a_hat, beta_b_hat)

    def gamma(self, k, d, alpha, beta):
        gamma = {}
        sum_seq = []
        for h in self.states:
            num = alpha[k][h] + beta[k][h]
            sum_seq.append(num)
            gamma[h] = num
        denom = logsumexp(sum_seq)
        
        for h in self.states:
            gamma[h] -= denom
        return gamma
    
    def gamma_partial(self, k, d, gamma):
        gamma_partial = {}
        for h in self.states:
            gamma_partial[h] = {}
            common = gamma[h]
            for yk in [0, 1]:
                gamma_partial[h][yk] = common + self.emission_probs(h, yk, d[k]) - self.emission_probs(h, None, d[k], marginalize=True)
        return gamma_partial
        
    def xi_matrix(self, k, d, alpha, beta):
        dk = d[k+1]

        xi = {}
        sum_seq = []
        for h in self.states:
            xi[h] = {}
            for m in self.states:
                num = alpha[k][h] + self.a[h][m] \
                    + self.emission_probs(m, None, dk, marginalize=True) + beta[k + 1][m]
                sum_seq.append(num)
                xi[h][m] = num
        denom = logsumexp(sum_seq)

        for h in self.states:
            for m in self.states:
                xi[h][m] -= denom
        return xi
    
    def tau(self, k, d, equi_pi):
        tau = {}
        for yk in [0, 1]:
            num, denom = [], []
            for h in self.states:
                num.append(self.emission_probs(h, yk, d[k]) + equi_pi[h])
                denom.append(self.emission_probs(h, None, d[k], marginalize=True) + equi_pi[h])
            num = logsumexp(num)
            denom = logsumexp(denom)
            tau[yk] = num - denom
        return tau

    def update(self, d, cutoff=1e-1):
        increase = cutoff + 1
        while (increase > cutoff):
            old_params = (self.pi, self.a, self.phi, self.beta_a, self.beta_b)
            before = self.forward_probability(self.forward(d))
            new_params = self.forward_backward(d)
            self.pi = new_params[0]
            self.a = new_params[1]
            self.phi = new_params[2]
            self.beta_a = new_params[3]
            self.beta_b = new_params[4]
            after = self.forward_probability(self.forward(d))
            increase = after - before
            print("log-likelihood:", after)
        
        if increase < 0:
            self.pi = old_params[0]
            self.a = old_params[1]
            self.phi = old_params[2]
            self.beta_a = old_params[3]
            self.beta_b = old_params[4]



class Oracle_BMM_HMM(BMM_HMM):
    def __init__(
            self,
            d, 
            init_pi, 
            init_a, 
            init_phi, 
            init_beta_a, 
            init_beta_b,
            tol=1e-1
    ):
        super().__init__(
                d, 
                init_pi, 
                init_a, 
                init_phi, 
                init_beta_a, 
                init_beta_b,
                tol
            )
        
    def update(self, d):
        "No need to update BMM-HMM if the oracle solutions are known."
        pass


class Constrained_BMM_HMM(BMM_HMM):
    def __init__(
            self,
            d, 
            init_pi, 
            init_a, 
            init_phi, 
            init_beta_a, 
            init_beta_b,
            pi_prior,
            a_prior,
            phi_prior,
            beta_a_prior,
            beta_b_prior,
            tol=1e-1
    ):
        super().__init__(
                d, 
                init_pi, 
                init_a, 
                init_phi, 
                init_beta_a, 
                init_beta_b,
                tol
            )
        self.pi_prior = pi_prior
        self.a_prior = a_prior
        self.phi_prior = phi_prior
        self.beta_a_prior = beta_a_prior
        self.beta_b_prior = beta_b_prior
        
    def bmm_neg_loglike(self, params):
        K = len(self.d)
        beta_a, beta_b = params[:2], params[2:]    

        lpdf = np.zeros((K, 2))
        lpdf[:,0] = stats.beta.logpdf(self.d, beta_a[0], beta_b[0])
        lpdf[:,1] = stats.beta.logpdf(self.d, beta_a[1], beta_b[1])

        tau = [np.exp(list(self.tau(k, self.d, self.equi_pi).values())) for k in range(K)]
        state_ll = [list(self.phi[h].values()) + self.equi_pi[h] for h in self.states]

        nll = [-1. * (logsumexp(state_ll + lpdf[k], 0) * tau[k]).sum() for k in range(K)]

        # dirichlet prior
        lp = stats.dirichlet.logpdf(np.exp(list(self.pi.values())), self.pi_prior)
        for h in self.states:
            lp += stats.dirichlet.logpdf(np.exp(list(self.a[h].values())), self.a_prior[h])
            lp += stats.dirichlet.logpdf(np.exp(list(self.phi[h].values())), self.phi_prior[h])

        # beta conjugate prior
        lp_beta = stats.expon.logpdf(beta_a[0], scale=1/self.beta_a_prior[0])
        lp_beta += stats.expon.logpdf(beta_b[1], scale=1/self.beta_b_prior[1])

        return np.sum(nll) + lp + lp_beta
    
    def forward_backward(self, d):
        
        # E step
        alpha = self.forward(d)
        beta = self.backward(d)
        K = len(d)

        xis = []
        for k in range(K - 1):
            xis.append(self.xi_matrix(k, d, alpha, beta))
            
        gammas = []
        for k in range(K):
            gammas.append(self.gamma(k, d, alpha, beta))
            
        gammas_partial = []
        for k in range(K):
            gammas_partial.append(self.gamma_partial(k, d, gammas[k]))

        # M step
        pi_hat = {}
        pi_denom = 0
        for h in self.states:
            pi_hat[h] = np.exp(gammas[0][h]) + self.pi_prior[h]
            pi_denom += pi_hat[h]
        
        for h in self.states:
            pi_hat[h] = np.log(pi_hat[h]) - np.log(pi_denom)
        
        a_hat = {}
        for h in self.states:
            a_hat[h] = {}
            sum_seq = []
            for k in range(K - 1):
                sum_seq.append(gammas[k][h])
            sum_seq = sum_seq + list(np.log(self.a_prior[h]))
            a_hat_denom = logsumexp(sum_seq) 

            for m in self.states:
                sum_seq = []
                for k in range(K - 1):
                    sum_seq.append(xis[k][h][m])
                sum_seq.append(log(self.a_prior[h][m]))
                a_hat_num = logsumexp(sum_seq)
                a_hat[h][m] = a_hat_num - a_hat_denom 
         
        phi_hat = {}
        for h in self.states:
            phi_hat[h] = {}
            sum_seq = []
            for k in range(K):
                sum_seq.append(gammas[k][h])
            sum_seq = sum_seq + list(np.log(self.phi_prior[h]))
            phi_hat_denom = logsumexp(sum_seq)
            for yk in [0, 1]:
                sum_seq = []
                for k in range(K):
                    sum_seq.append(gammas_partial[k][h][yk])
                sum_seq.append(log(self.phi_prior[h][yk]))
                phi_hat_num = logsumexp(sum_seq)
                phi_hat[h][yk] = phi_hat_num - phi_hat_denom
        
        
        init_params = np.hstack([self.beta_a, self.beta_b])
        self.equi_pi = self.equilibrium_probs(a=a_hat)
        res = minimize(self.bmm_neg_loglike, init_params, constraints=constraints, tol=self.tol,
                       options={'disp': False})
        print("BMM Convergence Achieved: ", res.success)
        beta_a_hat = [res.x[0], res.x[1]]
        beta_b_hat = [res.x[2], res.x[3]]
            
        return (pi_hat, a_hat, phi_hat, beta_a_hat, beta_b_hat)

    

   
